# Lab1: Back Propagation

1. Change hyperparameters in `config.json` 
    - **data_type**: data type, `linear` or `xor`
    - **epoch**: training epoch, default is `1000000`
    - **learning_rate**: learning rate, default is `0.1`
    - **activation**: activation function, `sigmoid` or `tanh` or `relu` or `leaky_relu`
    - **optimizer**: learning rate optimizer, no or `momentum` or `adagrad` or `adam`
2. Train and test the model
    
    ```python
    python Lab/Lab1_Back_Propagation/code/main.py
    ```
    
3. Show the result
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/6de32585-7a26-4d5a-8e90-780ef11924ad/d6bab985-8d86-4fee-a011-d658e0ccb456/Untitled.png)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/6de32585-7a26-4d5a-8e90-780ef11924ad/95e5d40d-b5a0-416e-966f-9d24470ace7b/Untitled.png)